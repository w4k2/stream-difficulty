{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EKSPERYMENT 1\n",
    "### Komputer.\n",
    "\n",
    "Część 1: Przygotowanie danych i trenowanie klasyfikatorów\n",
    "\n",
    "Część 2: Generacja strumieni i set-up\n",
    "\n",
    "Część 3: Badanie jakości w strumieniu o zmiennej strudności"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Część 1: Trenowanie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ./files/test_32x32.mat\n",
      "torch.Size([26032, 3, 32, 32])\n",
      "(26032,)\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Prepare trainig data\n",
    "train_data = torchvision.datasets.SVHN('./files/', \n",
    "                                  split='test', #Tak.\n",
    "                                  download=True)\n",
    "\n",
    "train_X = (torch.tensor(train_data.data)/255).to(torch.float)\n",
    "train_y = train_data.labels\n",
    "\n",
    "dataset = TensorDataset(torch.Tensor(train_X),torch.Tensor(train_y))\n",
    "dataloader = DataLoader(dataset, batch_size=64)\n",
    "\n",
    "print(train_X.shape)\n",
    "print(train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 0.18599488\n",
      "0 51 0.75039804\n",
      "0 71 0.80021524\n",
      "1 1 0.19139028\n",
      "1 51 0.7952793\n",
      "1 58 0.8006154\n",
      "2 1 0.19166751\n",
      "2 51 0.7975261\n",
      "2 53 0.800293\n",
      "3 1 0.18786322\n",
      "3 36 0.8010007\n",
      "4 1 0.18638173\n",
      "4 16 0.80085456\n",
      "5 1 0.18681403\n",
      "5 45 0.8028484\n"
     ]
    }
   ],
   "source": [
    "from architectures import CNN, CNN1_10_Network, CNN1_5_Network, CNN2_10_20_Network, CNN2_5_10_Network, CNN3_5_10_20_Network, FC_Network\n",
    "\n",
    "# Initialize and train classifiers\n",
    "max_training_epochs = 250\n",
    "training_support_level = 0.8\n",
    "\n",
    "clfs = [\n",
    "    CNN(architecure=FC_Network(img_depth=3, x_input_size=32)),\n",
    "    CNN(architecure=CNN1_5_Network(img_depth=3, x_input_size=32)),\n",
    "    CNN(architecure=CNN1_10_Network(img_depth=3, x_input_size=32)),\n",
    "    CNN(architecure=CNN2_5_10_Network(img_depth=3, x_input_size=32)),\n",
    "    CNN(architecure=CNN2_10_20_Network(img_depth=3, x_input_size=32)),\n",
    "    CNN(architecure=CNN3_5_10_20_Network(img_depth=3, x_input_size=32)),  \n",
    "]\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "for clf_id, clf in enumerate(clfs):\n",
    "    optimizer = torch.optim.SGD(clf.parameters(), lr=1e-2)\n",
    "\n",
    "    for e in range(max_training_epochs):\n",
    "        if e==0:\n",
    "            clf.train(dataloader, loss_fn, optimizer)\n",
    "        else:\n",
    "\n",
    "            proba = nn.Softmax(dim=1)( clf(train_X))\n",
    "            max_proba = torch.max(proba, dim=1)[0] \n",
    "            mean_proba = torch.mean(max_proba).detach().numpy() # średnie wsparcie decyzyjne\n",
    "\n",
    "            if mean_proba>training_support_level:\n",
    "                print(clf_id, e, mean_proba)\n",
    "                break\n",
    "            \n",
    "            clf.train(dataloader, loss_fn, optimizer)\n",
    "        \n",
    "        if e%50==1:\n",
    "            print(clf_id, e, mean_proba)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save classifers\n",
    "for c_id, c in enumerate(clfs):\n",
    "    torch.save(c, 'models/%i.pt' % c_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Część 2: Przygotowanie strumieni i CDoS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from ConditionalEvidenceStream import ConditionalEvidenceStream\n",
    "from utils import make_condition_map, mix_to_factor\n",
    "import concepts\n",
    "\n",
    "# Prepare streams\n",
    "n_chunks = 1000\n",
    "chunk_size = 200\n",
    "\n",
    "stream_data = torchvision.datasets.SVHN('./files/', \n",
    "                                  split='train', \n",
    "                                  download=True)\n",
    "\n",
    "X = torch.tensor(stream_data.data)/255\n",
    "y = stream_data.labels\n",
    "\n",
    "X_pca = PCA(n_components=0.8).fit_transform(X.reshape(X.shape[0],-1))\n",
    "X_pca -= np.mean(X_pca, axis=0)\n",
    "X_pca /= np.std(X_pca, axis=0)\n",
    "\n",
    "factor = mix_to_factor(X_pca)\n",
    "condition_map = make_condition_map(n_cycles=3,\n",
    "                                   n_concepts=500,\n",
    "                                   factor=factor,\n",
    "                                   factor_range=(0.1,0.9))\n",
    "\n",
    "cp = concepts.concept_proba(n_concepts=500,\n",
    "                            n_chunks=n_chunks,\n",
    "                            normalize=True,\n",
    "                            sigma=1)\n",
    "\n",
    "stream = ConditionalEvidenceStream(X, y,\n",
    "                                   condition_map.T,\n",
    "                                   cp,\n",
    "                                   chunk_size=chunk_size,\n",
    "                                   fragile=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
